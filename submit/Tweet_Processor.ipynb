{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Processor\n",
    "\n",
    "**CS109a**: Fall 2018\n",
    "\n",
    "**Authors**: Gordon Hew, Wenqin Hu, Blair Leduc\n",
    "\n",
    "**TF**: Ken Arnold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tweepy\n",
    "import sys\n",
    "import jsonpickle\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "from datetime import date, datetime, time\n",
    "import re\n",
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "from dateutil import tz\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_file = os.path.join('data','last_100_timeline_tweets.json.gz')\n",
    "tweets_urls_file = os.path.join('data','tweet_urls.json')\n",
    "\n",
    "normalized_users_df_file = os.path.join('tmp','users_df.pkl')\n",
    "normalized_tweets_df_file = os.path.join('tmp','tweets_df.pkl')\n",
    "normalized_users_df_gz_file = os.path.join('tmp','users_df.pkl.gz')\n",
    "normalized_tweets_df_gz_file = os.path.join('tmp','tweets_df.pkl.gz')\n",
    "\n",
    "final_users_df_file = os.path.join('tmp','users_final_df.pkl')\n",
    "final_tweets_df_file = os.path.join('tmp','tweets_final_df.pkl')\n",
    "final_users_df_gz_file = os.path.join('tmp','users_final_df.pkl.gz')\n",
    "final_tweets_df_gz_file = os.path.join('tmp','tweets_final_df.pkl.gz')\n",
    "\n",
    "botometer_result_1000 = os.path.join('data',\n",
    "                                     'botometer_result_1000random.json')\n",
    "\n",
    "run_url_expander = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract JSON Tweets into a Pandas Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Proceessing Tweet # 10000\n",
      "Finished Proceessing Tweet # 20000\n",
      "Finished Proceessing Tweet # 30000\n",
      "Finished Proceessing Tweet # 40000\n",
      "Finished Proceessing Tweet # 50000\n",
      "Finished Proceessing Tweet # 60000\n",
      "Finished Proceessing Tweet # 70000\n",
      "Finished Proceessing Tweet # 80000\n",
      "Finished Proceessing Tweet # 90000\n",
      "Finished Proceessing Tweet # 97854\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>id</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>statuses_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>21331.004000</td>\n",
       "      <td>1867.186000</td>\n",
       "      <td>1139.991000</td>\n",
       "      <td>3.423497e+17</td>\n",
       "      <td>25.115000</td>\n",
       "      <td>31638.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>39114.780324</td>\n",
       "      <td>7344.047951</td>\n",
       "      <td>4073.343137</td>\n",
       "      <td>4.482004e+17</td>\n",
       "      <td>92.364381</td>\n",
       "      <td>59896.719907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.023431e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1550.250000</td>\n",
       "      <td>108.750000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>4.943091e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2309.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7982.500000</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>397.500000</td>\n",
       "      <td>2.814042e+09</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9648.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23129.750000</td>\n",
       "      <td>1094.500000</td>\n",
       "      <td>908.250000</td>\n",
       "      <td>8.528221e+17</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>32057.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>383288.000000</td>\n",
       "      <td>100730.000000</td>\n",
       "      <td>85123.000000</td>\n",
       "      <td>1.070908e+18</td>\n",
       "      <td>1550.000000</td>\n",
       "      <td>624250.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       favourites_count  followers_count  friends_count            id  \\\n",
       "count       1000.000000      1000.000000    1000.000000  1.000000e+03   \n",
       "mean       21331.004000      1867.186000    1139.991000  3.423497e+17   \n",
       "std        39114.780324      7344.047951    4073.343137  4.482004e+17   \n",
       "min            0.000000         0.000000       0.000000  3.023431e+06   \n",
       "25%         1550.250000       108.750000     167.000000  4.943091e+08   \n",
       "50%         7982.500000       363.000000     397.500000  2.814042e+09   \n",
       "75%        23129.750000      1094.500000     908.250000  8.528221e+17   \n",
       "max       383288.000000    100730.000000   85123.000000  1.070908e+18   \n",
       "\n",
       "       listed_count  statuses_count  \n",
       "count   1000.000000     1000.000000  \n",
       "mean      25.115000    31638.967000  \n",
       "std       92.364381    59896.719907  \n",
       "min        0.000000        1.000000  \n",
       "25%        0.000000     2309.750000  \n",
       "50%        2.000000     9648.500000  \n",
       "75%       11.000000    32057.500000  \n",
       "max     1550.000000   624250.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coordinates</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>place</th>\n",
       "      <th>quoted_status.contributors</th>\n",
       "      <th>quoted_status.coordinates</th>\n",
       "      <th>quoted_status.favorite_count</th>\n",
       "      <th>...</th>\n",
       "      <th>retweeted_status.retweet_count</th>\n",
       "      <th>retweeted_status.user.favourites_count</th>\n",
       "      <th>retweeted_status.user.followers_count</th>\n",
       "      <th>retweeted_status.user.friends_count</th>\n",
       "      <th>retweeted_status.user.id</th>\n",
       "      <th>retweeted_status.user.listed_count</th>\n",
       "      <th>retweeted_status.user.statuses_count</th>\n",
       "      <th>retweeted_status.user.time_zone</th>\n",
       "      <th>retweeted_status.user.utc_offset</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>97854.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.785400e+04</td>\n",
       "      <td>1.981000e+04</td>\n",
       "      <td>2.087000e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.207000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>5.444200e+04</td>\n",
       "      <td>5.444200e+04</td>\n",
       "      <td>5.444200e+04</td>\n",
       "      <td>5.444200e+04</td>\n",
       "      <td>5.444200e+04</td>\n",
       "      <td>54442.000000</td>\n",
       "      <td>5.444200e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.785400e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.532150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.062750e+18</td>\n",
       "      <td>1.060179e+18</td>\n",
       "      <td>2.982529e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.133277e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.612598e+04</td>\n",
       "      <td>2.196059e+04</td>\n",
       "      <td>1.248257e+06</td>\n",
       "      <td>7.173465e+03</td>\n",
       "      <td>2.701214e+17</td>\n",
       "      <td>3002.657103</td>\n",
       "      <td>4.393735e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.349681e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>69.065511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.524701e+16</td>\n",
       "      <td>5.548095e+16</td>\n",
       "      <td>4.329941e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.518427e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>5.590547e+04</td>\n",
       "      <td>4.538322e+04</td>\n",
       "      <td>6.206854e+06</td>\n",
       "      <td>4.133332e+04</td>\n",
       "      <td>4.176677e+17</td>\n",
       "      <td>12902.510654</td>\n",
       "      <td>2.279119e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.445371e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.681337e+10</td>\n",
       "      <td>1.467126e+10</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.670000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.023431e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.068163e+18</td>\n",
       "      <td>1.067390e+18</td>\n",
       "      <td>2.214333e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.500000e+01</td>\n",
       "      <td>1.140000e+03</td>\n",
       "      <td>1.692000e+03</td>\n",
       "      <td>2.130000e+02</td>\n",
       "      <td>2.286717e+08</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.757000e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.934314e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.070557e+18</td>\n",
       "      <td>1.070352e+18</td>\n",
       "      <td>2.167074e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>6.170000e+02</td>\n",
       "      <td>6.890000e+03</td>\n",
       "      <td>1.258100e+04</td>\n",
       "      <td>6.100000e+02</td>\n",
       "      <td>1.708861e+09</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>1.413250e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.787828e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.071038e+18</td>\n",
       "      <td>1.070917e+18</td>\n",
       "      <td>8.150505e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.228200e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>7.240250e+03</td>\n",
       "      <td>2.422800e+04</td>\n",
       "      <td>1.102230e+05</td>\n",
       "      <td>1.944750e+03</td>\n",
       "      <td>7.785381e+17</td>\n",
       "      <td>747.000000</td>\n",
       "      <td>4.485200e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.468366e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>19464.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.071283e+18</td>\n",
       "      <td>1.071281e+18</td>\n",
       "      <td>1.071065e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.802781e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.668812e+06</td>\n",
       "      <td>1.500095e+06</td>\n",
       "      <td>1.034728e+08</td>\n",
       "      <td>1.650450e+06</td>\n",
       "      <td>1.071199e+18</td>\n",
       "      <td>229738.000000</td>\n",
       "      <td>3.512209e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.070908e+18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       coordinates  favorite_count  geo            id  in_reply_to_status_id  \\\n",
       "count          0.0    97854.000000  0.0  9.785400e+04           1.981000e+04   \n",
       "mean           NaN        1.532150  NaN  1.062750e+18           1.060179e+18   \n",
       "std            NaN       69.065511  NaN  4.524701e+16           5.548095e+16   \n",
       "min            NaN        0.000000  NaN  1.681337e+10           1.467126e+10   \n",
       "25%            NaN        0.000000  NaN  1.068163e+18           1.067390e+18   \n",
       "50%            NaN        0.000000  NaN  1.070557e+18           1.070352e+18   \n",
       "75%            NaN        0.000000  NaN  1.071038e+18           1.070917e+18   \n",
       "max            NaN    19464.000000  NaN  1.071283e+18           1.071281e+18   \n",
       "\n",
       "       in_reply_to_user_id  place  quoted_status.contributors  \\\n",
       "count         2.087000e+04    0.0                         0.0   \n",
       "mean          2.982529e+17    NaN                         NaN   \n",
       "std           4.329941e+17    NaN                         NaN   \n",
       "min           1.200000e+01    NaN                         NaN   \n",
       "25%           2.214333e+08    NaN                         NaN   \n",
       "50%           2.167074e+09    NaN                         NaN   \n",
       "75%           8.150505e+17    NaN                         NaN   \n",
       "max           1.071065e+18    NaN                         NaN   \n",
       "\n",
       "       quoted_status.coordinates  quoted_status.favorite_count      ...       \\\n",
       "count                        0.0                  5.207000e+03      ...        \n",
       "mean                         NaN                  4.133277e+04      ...        \n",
       "std                          NaN                  1.518427e+05      ...        \n",
       "min                          NaN                  0.000000e+00      ...        \n",
       "25%                          NaN                  1.400000e+01      ...        \n",
       "50%                          NaN                  7.000000e+02      ...        \n",
       "75%                          NaN                  1.228200e+04      ...        \n",
       "max                          NaN                  1.802781e+06      ...        \n",
       "\n",
       "       retweeted_status.retweet_count  retweeted_status.user.favourites_count  \\\n",
       "count                    5.444200e+04                            5.444200e+04   \n",
       "mean                     1.612598e+04                            2.196059e+04   \n",
       "std                      5.590547e+04                            4.538322e+04   \n",
       "min                      0.000000e+00                            0.000000e+00   \n",
       "25%                      3.500000e+01                            1.140000e+03   \n",
       "50%                      6.170000e+02                            6.890000e+03   \n",
       "75%                      7.240250e+03                            2.422800e+04   \n",
       "max                      1.668812e+06                            1.500095e+06   \n",
       "\n",
       "       retweeted_status.user.followers_count  \\\n",
       "count                           5.444200e+04   \n",
       "mean                            1.248257e+06   \n",
       "std                             6.206854e+06   \n",
       "min                             0.000000e+00   \n",
       "25%                             1.692000e+03   \n",
       "50%                             1.258100e+04   \n",
       "75%                             1.102230e+05   \n",
       "max                             1.034728e+08   \n",
       "\n",
       "       retweeted_status.user.friends_count  retweeted_status.user.id  \\\n",
       "count                         5.444200e+04              5.444200e+04   \n",
       "mean                          7.173465e+03              2.701214e+17   \n",
       "std                           4.133332e+04              4.176677e+17   \n",
       "min                           0.000000e+00              7.670000e+02   \n",
       "25%                           2.130000e+02              2.286717e+08   \n",
       "50%                           6.100000e+02              1.708861e+09   \n",
       "75%                           1.944750e+03              7.785381e+17   \n",
       "max                           1.650450e+06              1.071199e+18   \n",
       "\n",
       "       retweeted_status.user.listed_count  \\\n",
       "count                        54442.000000   \n",
       "mean                          3002.657103   \n",
       "std                          12902.510654   \n",
       "min                              0.000000   \n",
       "25%                             12.000000   \n",
       "50%                            102.000000   \n",
       "75%                            747.000000   \n",
       "max                         229738.000000   \n",
       "\n",
       "       retweeted_status.user.statuses_count  retweeted_status.user.time_zone  \\\n",
       "count                          5.444200e+04                              0.0   \n",
       "mean                           4.393735e+04                              NaN   \n",
       "std                            2.279119e+05                              NaN   \n",
       "min                            0.000000e+00                              NaN   \n",
       "25%                            3.757000e+03                              NaN   \n",
       "50%                            1.413250e+04                              NaN   \n",
       "75%                            4.485200e+04                              NaN   \n",
       "max                            3.512209e+07                              NaN   \n",
       "\n",
       "       retweeted_status.user.utc_offset       user_id  \n",
       "count                               0.0  9.785400e+04  \n",
       "mean                                NaN  3.349681e+17  \n",
       "std                                 NaN  4.445371e+17  \n",
       "min                                 NaN  3.023431e+06  \n",
       "25%                                 NaN  4.934314e+08  \n",
       "50%                                 NaN  2.787828e+09  \n",
       "75%                                 NaN  8.468366e+17  \n",
       "max                                 NaN  1.070908e+18  \n",
       "\n",
       "[8 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "users_json = list()\n",
    "tweets_json = list()\n",
    "\n",
    "with gzip.open(tweets_file, 'rt') as f:\n",
    "    idx = 0\n",
    "    for idx, line in enumerate(f):\n",
    "        line_json = json.loads(line)\n",
    "\n",
    "        if 'user' in line_json:\n",
    "            user_json = line_json.pop('user', None)\n",
    "            users_json.append(user_json)\n",
    "\n",
    "            line_json['user_id'] = user_json['id']\n",
    "            line_json['id_str'] = user_json['id_str']\n",
    "            line_json['screen_name'] = user_json['screen_name']\n",
    "\n",
    "        tweets_json.append(line_json)\n",
    "\n",
    "        if ((idx + 1) % 10000 == 0):\n",
    "            print('Finished Proceessing Tweet #', idx + 1)\n",
    "\n",
    "    print('Finished Proceessing Tweet #', idx + 1)\n",
    "\n",
    "users_df = json_normalize(users_json)\n",
    "tweets_df = json_normalize(tweets_json)\n",
    "\n",
    "display(users_df.describe())\n",
    "display(tweets_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Write as binary file\n",
    "users_df.to_pickle(normalized_users_df_gz_file, compression = 'gzip')\n",
    "tweets_df.to_pickle(normalized_tweets_df_gz_file, compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users_pkl_df = pd.read_pickle(normalized_users_df_gz_file, \n",
    "                              compression = 'gzip')\n",
    "tweets_pkl_df = pd.read_pickle(normalized_tweets_df_gz_file, \n",
    "                               compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(users_pkl_df.describe())\n",
    "display(users_pkl_df.head())\n",
    "display(tweets_pkl_df.describe())\n",
    "display(tweets_pkl_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_pkl_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website Classification Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Website URLs From Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pkl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_urls_df = tweets_pkl_df[tweets_pkl_df['entities.urls']\\\n",
    "                                    .map(lambda urls: len(urls)) > 0]\n",
    "\n",
    "print('tweets with links:', len(tweets_with_urls_df.index))\n",
    "display(tweets_with_urls_df.head())\n",
    "display(tweets_with_urls_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if run_url_expander:\n",
    "    session = requests.Session()\n",
    "\n",
    "    tweet_urls = list()\n",
    "\n",
    "    for i,(tweet_idx, row) in enumerate(tweets_with_urls_df.iterrows()):\n",
    "        # visit the URL and get the expanded site\n",
    "        for url_idx, url in enumerate(row['entities.urls']):\n",
    "            try:\n",
    "                resp = session.head(url['expanded_url'], \n",
    "                                    allow_redirects = True)\n",
    "                tweet_urls.append({'expanded_url': url['expanded_url'], \n",
    "                                   'unshortened_url': resp.url })\n",
    "            except Exception as err:\n",
    "                print('Unable to process url', url['expanded_url'], err)\n",
    "\n",
    "        if ((i + 1) % 100 == 0):\n",
    "            print('Finished Proceessing Tweet #', i + 1)\n",
    "\n",
    "    print('Finished Proceessing Tweet #', i + 1)\n",
    "\n",
    "    with open('tweet_urls.json', 'w') as f:\n",
    "        for url_json in tweet_urls:\n",
    "            f.write(json.dumps(url_json))\n",
    "            f.write('\\n')\n",
    "\n",
    "    print('Processed', len(tweet_urls), 'URLs Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Websites Using Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_website(url):\n",
    "    \n",
    "    is_twitter = url.startswith('https://twitter.com') \\\n",
    "        or url.startswith('https://mobile.twitter.com')\n",
    "    \n",
    "    if is_twitter:\n",
    "        return 'Twitter'\n",
    "    \n",
    "    other_social_media_urls = (\n",
    "        'https://i.imgur.com',\n",
    "        'https://www.instagram.com',\n",
    "        'https://www.facebook.com',\n",
    "        'https://m.soundcloud.com',\n",
    "        'https://soundcloud.com',\n",
    "        'https://www.linkedin.com',\n",
    "        'https://vine.co',\n",
    "        'http://www.facebook.com',\n",
    "    )\n",
    "    \n",
    "    if url.startswith(other_social_media_urls):\n",
    "        return 'Other Social Media'\n",
    "    \n",
    "    digital_media_urls = (\n",
    "        'https://www.twitch.tv',\n",
    "        'https://www.youtube.com',\n",
    "        'https://open.spotify.com/',\n",
    "        'http://www.youtube.com',\n",
    "        'http://mpg.dnset.com',\n",
    "        'https://www.pscp.tv',\n",
    "    )\n",
    "    \n",
    "    if url.startswith(digital_media_urls):\n",
    "        return 'Digital Media'\n",
    "    \n",
    "    news_urls = (\n",
    "        'https://www.nytimes.com',\n",
    "        'https://www.newsweek.com',\n",
    "        'https://abc7.com',\n",
    "        'https://news.sky.com',\n",
    "        'https://www.bbc.com',\n",
    "        'https://www.sbs.com.au',\n",
    "        'https://www.yahoo.com/news/',\n",
    "        'https://www.yahoo.com/lifestyle/',\n",
    "        'https://abcnews.go.com',\n",
    "        'https://www.wsj.com',\n",
    "        'https://thehill.com',\n",
    "        'https://www.foxnews.com',\n",
    "        'https://www.usatoday.com',\n",
    "        'https://www.sun-sentinel.com',\n",
    "        'https://www.washingtontimes.com',\n",
    "        'https://www.washingtonpost.com',\n",
    "        'http://time.com',\n",
    "        'https://fox8.com',\n",
    "        'https://www.huffingtonpost.com',\n",
    "        'http://asiannewsservice.in',\n",
    "        'http://austin.culturemap.com',\n",
    "        'https://www.reuters.com',\n",
    "        \n",
    "        # business\n",
    "        'https://www.forbes.com',\n",
    "        'https://www.zerohedge.com',\n",
    "        'https://thebrandboy.com',\n",
    "        \n",
    "        # Gaming\n",
    "        'https://www.gameinformer.com',\n",
    "        'https://mynintendonews.com',\n",
    "        \n",
    "        # Tech\n",
    "        'https://www.engadget.com',\n",
    "        'https://arstechnica.com',\n",
    "        'https://www.zdnet.com',\n",
    "        \n",
    "        'https://www.travelandleisure.com',\n",
    "        'http://dallas.culturemap.com',\n",
    "        \n",
    "        'https://www.statesman.com',\n",
    "        \n",
    "        'https://www.washingtonexaminer.com',\n",
    "        'https://www.dailywire.com',\n",
    "        'https://www.dailymail.co.uk',\n",
    "        'http://www.dailymail.co.uk',\n",
    "        'https://www.theblaze.com',\n",
    "        'https://www.breitbart.com',\n",
    "        'https://dailycaller.com',\n",
    "        'https://paper.li',\n",
    "        'http://www.kohimanewspaper.org',\n",
    "        'http://geteducation.com.au',\n",
    "        'https://morgan-magazine.com',\n",
    "        'http://www.travelweekly.co.uk',\n",
    "        'https://apple.news',\n",
    "        'http://getstem.com.au',\n",
    "        'https://www.bloomberg.com',\n",
    "        'http://thefederalist.com',\n",
    "        'http://getindustry.com.au',\n",
    "        'https://www.thedailybeast.com',\n",
    "        'https://www.hannity.com',\n",
    "        'https://www.cnbc.com',\n",
    "        'https://hillreporter.com',\n",
    "        'https://www.politicususa.com',\n",
    "        'https://www.mesopinions.com',\n",
    "        'https://www.theguardian.com',\n",
    "        'https://www.cpr.org',\n",
    "        'https://www.cnn.com',\n",
    "        'https://www.thegatewaypundit.com',\n",
    "    )\n",
    "    \n",
    "    if url.startswith(news_urls):\n",
    "        return 'News and Current Events'\n",
    "    \n",
    "    commercial_product = (\n",
    "        'https://store.playstation.com',\n",
    "        'https://www.starz.com',\n",
    "        'https://www.amazon.com',\n",
    "        'https://represent.com/store/pewdiepie',\n",
    "        'https://itunes.apple.com',\n",
    "        'https://sumall.com',\n",
    "        'http://fllwrs.com/',\n",
    "        'http://www.tweematic.com',\n",
    "        'https://www.bible.com/', # sells bible apps\n",
    "        'https://www.xbox.com',\n",
    "        'https://www1.ticketmaster.com',\n",
    "        'https://www.zazzle.com',\n",
    "        'https://www.woodenrings.com',\n",
    "        'https://www.workable.com',\n",
    "        'http://www.twitlonger.com',\n",
    "        \n",
    "        'https://www.twittascope.com',\n",
    "        'https://pages.ebay.com',\n",
    "        'https://www.ebay.co.uk',\n",
    "        'https://curiouscat.me',\n",
    "        'https://api.curations.bazaarvoice.com',\n",
    "        'https://www.exclusivetravelrates.com',\n",
    "        'https://poshmark.com',\n",
    "        'http://foodtronic.com',\n",
    "        'https://www.etsy.com',\n",
    "        'https://www.commissioncut.ca',\n",
    "        'http://www.edumine.com',\n",
    "        'http://smarturl.it',\n",
    "        'http://naver.me',\n",
    "        'https://www.gofundme.com',\n",
    "        'https://www.documentcloud.org',\n",
    "        'https://mailchi.mp',\n",
    "        'https://fanlink.to',\n",
    "        'http://nocturnalvegas.com',\n",
    "        'https://www.bandsintown.com',\n",
    "        'https://trakt.tv',\n",
    "        'https://www.crowdfireapp.com',\n",
    "        'https://gifkaro.app.link',\n",
    "        'https://empire.lnk.to',\n",
    "        'https://www.careerwebsite.com',\n",
    "        'http://epphany.com',\n",
    "    )\n",
    "    \n",
    "    if url.startswith(commercial_product):\n",
    "        return 'Commercial Product & Services'\n",
    "    \n",
    "    celebrities_urls = (\n",
    "        'https://people.com',\n",
    "        'https://www.hollywoodreporter.com',\n",
    "        'https://www.allkpop.com',\n",
    "        'https://www.tmz.com',\n",
    "        'https://ew.com',\n",
    "        'https://www.out.com',\n",
    "        'https://www.soompi.com',\n",
    "        'https://www.billboard.com',\n",
    "        'https://www.telltaletv.com',\n",
    "        'https://entertain.naver.com',\n",
    "        'https://onlyfans.com',\n",
    "        'https://starcinema.abs-cbn.com',\n",
    "        'https://bongino.com',\n",
    "        'http://exo.smtown.com',\n",
    "        'https://www.complex.com',\n",
    "    )\n",
    "    \n",
    "    if url.startswith(celebrities_urls):\n",
    "        return 'Celebrities'\n",
    "    \n",
    "    organizations_urls = (\n",
    "        'https://www.nasa.gov',\n",
    "        'https://www.justice.gov',\n",
    "        'http://www.pewresearch.org',\n",
    "        'https://nca2018.globalchange.gov',\n",
    "        'http://cc.com/vote',\n",
    "        'https://www.patriotguard.org',\n",
    "        'https://www.weforum.org',\n",
    "        'https://forcechange.com',\n",
    "        'https://animalpetitions.org',\n",
    "    )\n",
    "    \n",
    "    if url.startswith(organizations_urls):\n",
    "        return 'Organization'    \n",
    "    \n",
    "    sports_urls = (\n",
    "        'https://www.clevelandbrowns.com',\n",
    "        'https://www.sbnation.com',\n",
    "        'https://mlb.nbcsports.com',\n",
    "        'https://silverandblacktoday.com',\n",
    "        'https://www.mlb.com',\n",
    "        'http://www.lpga.com',\n",
    "        'https://www.wwe.com',\n",
    "        'https://www.lpga.or.jp',\n",
    "        'https://www.nhl.com',\n",
    "    )\n",
    "    \n",
    "    if url.startswith(sports_urls):\n",
    "        return 'Sports'  \n",
    "    \n",
    "    adult_urls = (\n",
    "        'https://justfor.fans',\n",
    "        'https://id10893.weebly.com',\n",
    "        'https://www.manyvids.com',\n",
    "    )\n",
    "    \n",
    "    if url.startswith(adult_urls):\n",
    "        return 'Adult Content'\n",
    "    \n",
    "    unknown_urls = (\n",
    "        'https://t1.daumcdn.net',\n",
    "        'https://t.hrtye.com',\n",
    "    )\n",
    "    \n",
    "    if url.startswith(unknown_urls):\n",
    "        return 'Unknown' \n",
    "    \n",
    "    return 'Uncategorized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "urls = list()\n",
    "\n",
    "with open(tweets_urls_file, 'r') as f:\n",
    "    for url_json in f:\n",
    "        url_json = json.loads(url_json)\n",
    "        \n",
    "        url_json['category'] \\\n",
    "            = categorize_website(url_json['unshortened_url'])\n",
    "        \n",
    "        unshortened_url = url_json['unshortened_url']\n",
    "        \n",
    "        if unshortened_url.startswith('https://'):\n",
    "            begin_index = len('https://')\n",
    "        else:\n",
    "            begin_index = len('http://')\n",
    "        \n",
    "        try:\n",
    "            index = unshortened_url.index('/', begin_index)\n",
    "            url_json['domain'] = unshortened_url[:index]\n",
    "        except:\n",
    "            url_json['domain'] = unshortened_url\n",
    "        \n",
    "        urls.append(url_json)\n",
    "        \n",
    "urls_df = pd.DataFrame(urls)        \n",
    "\n",
    "grouped = urls_df.groupby(['domain', 'category'])\n",
    "\n",
    "top_100_websites_df = pd.DataFrame({'count':urls_df\\\n",
    "                                    .groupby([\"domain\", \"category\"])\\\n",
    "                                    .size()})\\\n",
    "                                    .nlargest(100, 'count')\\\n",
    "                                    .reset_index()\n",
    "display(top_100_websites_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls_df[['expanded_url', 'unshortened_url']].to_dict('series')\n",
    "urls_dict = pd.Series(urls_df.unshortened_url.values, \n",
    "                      index = urls_df.expanded_url).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T04:25:34.120025Z",
     "start_time": "2018-12-12T04:25:33.997950Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets_url_cat_df = tweets_pkl_df.copy(deep = True)\n",
    "\n",
    "tweets_url_cat_df['links_to_twitter'] = 0\n",
    "tweets_url_cat_df['links_to_top_social_media'] = 0\n",
    "tweets_url_cat_df['links_to_top_digital_media'] = 0\n",
    "tweets_url_cat_df['links_to_top_news'] = 0\n",
    "tweets_url_cat_df['links_to_top_products_services'] = 0\n",
    "tweets_url_cat_df['links_to_top_celebrities'] = 0\n",
    "tweets_url_cat_df['links_to_top_organizations'] = 0\n",
    "tweets_url_cat_df['links_to_top_sports'] = 0\n",
    "tweets_url_cat_df['links_to_top_adult'] = 0\n",
    "\n",
    "for i, (tweet_idx, row) in enumerate(tweets_url_cat_df.iterrows()):\n",
    "    # visit the URL and get the expanded site\n",
    "    \n",
    "    if 'entities.urls' in row and len(row['entities.urls']) > 0:\n",
    "    \n",
    "        for url_idx, url in enumerate(row['entities.urls']):           \n",
    "            \n",
    "            if url['expanded_url'] in urls_dict:\n",
    "            \n",
    "                unshortened_url = urls_dict[url['expanded_url']]\n",
    "                category = categorize_website(unshortened_url)\n",
    "\n",
    "                if category == 'Twitter':\n",
    "                    row['links_to_twitter'] = row['links_to_twitter']+1\n",
    "                    tweets_url_cat_df.loc[tweet_idx, \n",
    "                                          'links_to_twitter'] \\\n",
    "                                            = row['links_to_twitter']\n",
    "                elif category == 'Other Social Media':\n",
    "                    row['links_to_top_social_media'] \\\n",
    "                            = row['links_to_top_social_media'] + 1\n",
    "                    tweets_url_cat_df.loc[tweet_idx, \n",
    "                                'links_to_top_social_media'] \\\n",
    "                                    = row['links_to_top_social_media']\n",
    "                elif category == 'Digital Media':\n",
    "                    row['links_to_top_digital_media'] \\\n",
    "                        = row['links_to_top_digital_media'] + 1\n",
    "                    tweets_url_cat_df.loc[tweet_idx, \n",
    "                                'links_to_top_digital_media'] \\\n",
    "                                    = row['links_to_top_digital_media']              \n",
    "                elif category == 'News and Current Events':\n",
    "                    row['links_to_top_news']=row['links_to_top_news']+1\n",
    "                    tweets_url_cat_df.loc[tweet_idx, \n",
    "                                          'links_to_top_news'] \\\n",
    "                                            = row['links_to_top_news']              \n",
    "                elif category == 'Commercial Product & Services':\n",
    "                    row['links_to_top_products_services'] \\\n",
    "                        = row['links_to_top_products_services'] + 1\n",
    "                    tweets_url_cat_df.loc[tweet_idx, \n",
    "                            'links_to_top_products_services'] \\\n",
    "                                = row['links_to_top_products_services']           \n",
    "                elif category == 'Celebrities':\n",
    "                    row['links_to_top_celebrities'] \\\n",
    "                        = row['links_to_top_celebrities'] + 1\n",
    "                    tweets_url_cat_df.loc[tweet_idx, \n",
    "                                'links_to_top_celebrities'] \\\n",
    "                                    = row['links_to_top_celebrities']\n",
    "                elif category == 'Organization':\n",
    "                    row['links_to_top_organizations'] \\\n",
    "                        = row['links_to_top_organizations'] + 1\n",
    "                    tweets_url_cat_df.loc[tweet_idx, \n",
    "                                'links_to_top_organizations'] \\\n",
    "                                    = row['links_to_top_organizations']          \n",
    "                elif category == 'Sports':\n",
    "                    row['links_to_top_sports'] \\\n",
    "                        = row['links_to_top_sports'] + 1\n",
    "                    tweets_url_cat_df.loc[tweet_idx, \n",
    "                                        'links_to_top_sports'] \\\n",
    "                                            = row['links_to_top_sports']            \n",
    "                elif category == 'Adult Content':\n",
    "                    row['links_to_top_adult'] \\\n",
    "                        = row['links_to_top_adult'] + 1\n",
    "                    tweets_url_cat_df.loc[tweet_idx, \n",
    "                                    'links_to_top_adult'] \\\n",
    "                                            = row['links_to_top_adult']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append Bot-o-meter Score to User Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(botometer_result_1000, 'rt') as bot_read:\n",
    "    botometer_result = json.load(bot_read)\n",
    "    \n",
    "botometer_results_df = json_normalize(botometer_result)\n",
    "users_final_df = users_pkl_df.copy(deep = True)\n",
    "botometer_results_df = botometer_results_df[['user.id_str', \n",
    "                                             'scores.universal']]\n",
    "users_final_df = users_final_df.merge(botometer_results_df, \n",
    "                                      left_on='id_str', \n",
    "                                      right_on='user.id_str', \n",
    "                                      how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append Bot-o-meter Score to Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_final_df = tweets_url_cat_df.copy(deep = True)\n",
    "botometer_results_df = botometer_results_df[['user.id_str', \n",
    "                                             'scores.universal']]\n",
    "tweets_final_df = tweets_final_df.merge(botometer_results_df, \n",
    "                                        left_on='id_str', \n",
    "                                        right_on='user.id_str', \n",
    "                                        how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out Final Data Frames for Aggregation Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_final_df.to_pickle(final_users_df_gz_file, compression = 'gzip')\n",
    "tweets_final_df.to_pickle(final_tweets_df_gz_file, compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
